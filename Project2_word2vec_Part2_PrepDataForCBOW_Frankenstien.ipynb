{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2_word2vec-Part2-PrepDataForCBOW-Frankenstien.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP04z98OzSdbSF10qMkTyTl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talktovishal/NLP-ML-Projects/blob/master/Project2_word2vec_Part2_PrepDataForCBOW_Frankenstien.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7Ll0r4oMntK",
        "colab_type": "code",
        "outputId": "651886f2-e16d-4076-d877-8fb7dc26d1aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pwd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTYPYS8dNjGT",
        "colab_type": "code",
        "outputId": "6a6e0b38-69be-4a44-a8aa-8e545be46bc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive/Public Shared\"\n",
        "!cp \"/content/drive/My Drive/Public Shared/Project-2-Word-Embeddings/normalizedSentences-frankenstein.pickle\" \"normalizedSentences.pickle\"\n",
        "#!cp \"/content/drive/My Drive/Public Shared/Project-2-Word-Embeddings/mini-normalizedSentences.pickle\" \"normalizedSentences.pickle\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Project-2-Word-Embeddings  'vishalc final certificate.pdf'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSrn_RAxOI4L",
        "colab_type": "code",
        "outputId": "9a916ff6-97c8-4eb8-d549-fa0dfa401a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "import time\n",
        "\n",
        "t = time.process_time()\n",
        "# By choosing to open the file in mode wb, you are choosing to write in raw binary. There is no character encoding being applied.\n",
        "# Thus to read this file, you should simply open in mode rb.\n",
        "with open('normalizedSentences.pickle', 'rb') as f:\n",
        "  normalizedSentences = pickle.load(f)\n",
        "\n",
        "print(time.process_time() - t)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.005084805000000081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXw_vfEsOd8B",
        "colab_type": "code",
        "outputId": "0255318b-3b64-4e6f-b049-7b7e9d0abc3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(normalizedSentences)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhIQmucdO8a-",
        "colab_type": "code",
        "outputId": "2c8e47d6-4583-4d93-9148-92a3aa188525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "normalizedSentences[345]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it is even possible that the train of my ideas would never have received the fatal impulse that led to my ruin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FT5uQm0SXAL",
        "colab_type": "code",
        "outputId": "389d7ec5-63fb-44a4-dd57-9732bedb8b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "start_time = time.process_time()\n",
        "#for stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "#wordsInSentenceWithStopWords = [word_tokenize(s) for s in normalizedSentences]\n",
        "#remove stop words else you have a high degree of predicting the, of, a etc.\n",
        "wordsInSentence = [tuple(w for w in word_tokenize(s) if not w in stop_words) for s in normalizedSentences]\n",
        "print(time.process_time() - start_time)\n",
        "#WHY is tuple needed in the expression above?\n",
        "#https://dev.to/vmesel/what-the-heck-are-python-generator-expressions-and-list-comprehensions-55hh\n",
        "#https://stackoverflow.com/questions/27968339/python-why-does-list-comprehension-produce-a-generator\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "0.5316614729999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP0Nhvkf5N2b",
        "colab_type": "code",
        "outputId": "0b6a40a0-9855-47c6-8133-83db8ffd5204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "len(wordsInSentence)\n",
        "\n",
        "wordsInSentence[0]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('frankenstein',\n",
              " 'modern',\n",
              " 'prometheus',\n",
              " 'mary',\n",
              " 'wollstonecraft',\n",
              " 'godwin',\n",
              " 'shelley',\n",
              " 'letter',\n",
              " 'st',\n",
              " 'petersburgh',\n",
              " 'dec',\n",
              " 'th',\n",
              " 'mrs',\n",
              " 'saville',\n",
              " 'england',\n",
              " 'rejoice',\n",
              " 'hear',\n",
              " 'disaster',\n",
              " 'accompanied',\n",
              " 'commencement',\n",
              " 'enterprise',\n",
              " 'regarded',\n",
              " 'evil',\n",
              " 'forebodings')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTmdf7I_SZS8",
        "colab_type": "code",
        "outputId": "e6ca83b4-e35f-441d-ae95-5d63bbf67742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#get the vocab\n",
        "vocab = set()\n",
        "for words in wordsInSentence:\n",
        "  vocab.update(words)\n",
        "\n",
        "len(vocab)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiV2CxozSozb",
        "colab_type": "code",
        "outputId": "a06ae305-a7f5-48dd-992b-05c05d3266c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import random\n",
        "for i, val in enumerate(random.sample(vocab, 10)):\n",
        "  print(val)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "foldings\n",
            "neater\n",
            "fettered\n",
            "periods\n",
            "liberal\n",
            "waters\n",
            "clasped\n",
            "avenge\n",
            "indulged\n",
            "weeks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "273V3xHtSrBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxSY1z8tSttc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "data = []\n",
        "for raw_text in wordsInSentence:\n",
        "  for i in range(2, len(raw_text) - 2):\n",
        "      #create numpy array instead of list as it helps later when converting into pytorch tensors\n",
        "      context = np.array([\n",
        "                 word_to_ix[raw_text[i - 2]], \n",
        "                 word_to_ix[raw_text[i - 1]],\n",
        "                 word_to_ix[raw_text[i + 1]], \n",
        "                 word_to_ix[raw_text[i + 2]]\n",
        "                 ])\n",
        "      target = word_to_ix[raw_text[i]]\n",
        "      data.append((context, target))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_QhJMowSx1G",
        "colab_type": "code",
        "outputId": "d6ffea40-b5f2-4214-ec0f-35f87723b82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(data[:5])  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(array([2693, 3277, 5013, 5680]), 4122), (array([3277, 4122, 5680, 4033]), 5013), (array([4122, 5013, 4033, 1593]), 5680), (array([5013, 5680, 1593, 2149]), 4033), (array([5680, 4033, 2149, 4812]), 1593)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrMX-Q0JS1MU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "cbow_data = pd.DataFrame(data, columns = ['Context', 'Target'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf1fCHn8S6-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cbow_data.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v0ZpfzKS_4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(cbow_data)\n",
        "train_proportion=0.9\n",
        "val_proportion=0.05\n",
        "test_proportion=0.05\n",
        "\n",
        "#randomization.\n",
        "cbow_data = cbow_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def get_split(row_num):\n",
        "    if row_num <= n*train_proportion:\n",
        "        return 'train'\n",
        "    elif (row_num > n*train_proportion) and (row_num <= n*train_proportion + n*val_proportion):\n",
        "        return 'val'\n",
        "    else:\n",
        "        return 'test'\n",
        "        \n",
        "cbow_data['split']= cbow_data.apply(lambda row: get_split(row.name), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjiAmYieT_xU",
        "colab_type": "code",
        "outputId": "33db83d8-dae1-409a-fbb9-eabd43b1b029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "cbow_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Context</th>\n",
              "      <th>Target</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[3774, 168, 1108, 5341]</td>\n",
              "      <td>4763</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[2491, 1115, 158, 2321]</td>\n",
              "      <td>2691</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[6284, 6712, 3939, 3213]</td>\n",
              "      <td>1698</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[4055, 3083, 4673, 6318]</td>\n",
              "      <td>5028</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1178, 3528, 6436, 5558]</td>\n",
              "      <td>5200</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Context  Target  split\n",
              "0   [3774, 168, 1108, 5341]    4763  train\n",
              "1   [2491, 1115, 158, 2321]    2691  train\n",
              "2  [6284, 6712, 3939, 3213]    1698  train\n",
              "3  [4055, 3083, 4673, 6318]    5028  train\n",
              "4  [1178, 3528, 6436, 5558]    5200  train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcfmCkZxUSHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "with open('cbow_data.pickle', 'wb') as f:\n",
        "    pickle.dump(cbow_data, f)\n",
        "\n",
        "with open('word_to_ix.pickle', 'wb') as f:\n",
        "    pickle.dump(word_to_ix, f)\n",
        "\n",
        "with open('vocab.pickle', 'wb') as f:\n",
        "    pickle.dump(vocab, f)\n",
        "\"\"\"    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qPwuDtMUoRW",
        "colab_type": "code",
        "outputId": "483908c4-9ac3-413d-f167-fd8edd5dd904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(cbow_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23307"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OntD_TXQs_M",
        "colab_type": "code",
        "outputId": "0512dcc8-2c83-4c6f-87ed-6b59cf892433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "training_data = cbow_data[cbow_data['split'] == 'train']\n",
        "training_data.head()\n",
        "#training_data.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Context</th>\n",
              "      <th>Target</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[3774, 168, 1108, 5341]</td>\n",
              "      <td>4763</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[2491, 1115, 158, 2321]</td>\n",
              "      <td>2691</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[6284, 6712, 3939, 3213]</td>\n",
              "      <td>1698</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[4055, 3083, 4673, 6318]</td>\n",
              "      <td>5028</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1178, 3528, 6436, 5558]</td>\n",
              "      <td>5200</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Context  Target  split\n",
              "0   [3774, 168, 1108, 5341]    4763  train\n",
              "1   [2491, 1115, 158, 2321]    2691  train\n",
              "2  [6284, 6712, 3939, 3213]    1698  train\n",
              "3  [4055, 3083, 4673, 6318]    5028  train\n",
              "4  [1178, 3528, 6436, 5558]    5200  train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRmJhFOQU2Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#structure copied from https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IBSOPar_y2K",
        "colab_type": "code",
        "outputId": "4aab15a8-f42d-4c68-a213-15cff21e9373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "#batch size == 1. VERY VERY SLOW\n",
        "\"\"\"\n",
        "def make_context_vector(context_ids):\n",
        "    return torch.tensor(context_ids, dtype=torch.long)\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_layer = 128):\n",
        "      super(CBOW, self).__init__()\n",
        "      self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "      self.linear1 = nn.Linear(in_features=embedding_dim, out_features=hidden_layer)\n",
        "      self.linear2 = nn.Linear(in_features=hidden_layer, out_features=vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      x_embedded_sum = sum(self.embeddings(inputs)).view((1,-1))\n",
        "      # https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch\n",
        "      # What is the meaning of parameter -1?\n",
        "      # If there is any situation that you don't know how many rows you want but are sure of the number \n",
        "      # of columns, then you can specify this with a -1. (Note that you can extend this to tensors with \n",
        "      # more dimensions. Only one of the axis value can be -1). This is a way of telling the library: \n",
        "      # \"give me a tensor that has these many columns and you compute the appropriate number of rows that \n",
        "      # is necessary to make this happen\".      \n",
        "      out1 = F.relu(self.linear1(x_embedded_sum))\n",
        "      out2 = self.linear2(out1)\n",
        "      log_probs = F.log_softmax(out2, dim=1)\n",
        "      # axis : axis along which we want to calculate the sum value. Otherwise, it will consider arr to be \n",
        "      # flattened(works on all the axis). axis = 0 means along the column and axis = 1 means working along \n",
        "      # the row.\n",
        "      return log_probs\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "      word = torch.LongTensor([word_to_ix[word]])\n",
        "      return self.embeddings(word).view(1,-1)      \n",
        "\n",
        "\n",
        "counter = 0\n",
        "losses = []\n",
        "\n",
        "EMBEDDING_DIM = 99\n",
        "model = CBOW(vocab_size=len(vocab), embedding_dim=EMBEDDING_DIM)\n",
        "loss_funtion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "for epoch in range(50):\n",
        "  total_loss = 0\n",
        "  #iterate over the rows in df -- note doing one at a time, not batches yet.\n",
        "  for index, current_row in training_data.head(n=10000).iterrows():\n",
        "    # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "    # into integer indices and wrap them in tensors)    \n",
        "    context_vector = make_context_vector(current_row['Context'])\n",
        "    model.zero_grad()\n",
        "    log_probs = model(context_vector)\n",
        "    loss = loss_funtion(log_probs, torch.tensor([current_row['Target']], dtype=torch.long))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "    total_loss += loss.item()\n",
        "    counter += 1\n",
        "    # if(counter % 1000 == 0):\n",
        "    #   print(total_loss)\n",
        "  print(f\"{epoch}:{loss.item()}\")\n",
        "  losses.append(total_loss)\n",
        "\n",
        "print(losses)\n",
        "\"\"\"\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef make_context_vector(context_ids):\\n    return torch.tensor(context_ids, dtype=torch.long)\\n\\nclass CBOW(nn.Module):\\n\\n    def __init__(self, vocab_size, embedding_dim, hidden_layer = 128):\\n      super(CBOW, self).__init__()\\n      self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\\n      self.linear1 = nn.Linear(in_features=embedding_dim, out_features=hidden_layer)\\n      self.linear2 = nn.Linear(in_features=hidden_layer, out_features=vocab_size)\\n\\n\\n    def forward(self, inputs):\\n      x_embedded_sum = sum(self.embeddings(inputs)).view((1,-1))\\n      # https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch\\n      # What is the meaning of parameter -1?\\n      # If there is any situation that you don\\'t know how many rows you want but are sure of the number \\n      # of columns, then you can specify this with a -1. (Note that you can extend this to tensors with \\n      # more dimensions. Only one of the axis value can be -1). This is a way of telling the library: \\n      # \"give me a tensor that has these many columns and you compute the appropriate number of rows that \\n      # is necessary to make this happen\".      \\n      out1 = F.relu(self.linear1(x_embedded_sum))\\n      out2 = self.linear2(out1)\\n      log_probs = F.log_softmax(out2, dim=1)\\n      # axis : axis along which we want to calculate the sum value. Otherwise, it will consider arr to be \\n      # flattened(works on all the axis). axis = 0 means along the column and axis = 1 means working along \\n      # the row.\\n      return log_probs\\n\\n    def get_word_emdedding(self, word):\\n      word = torch.LongTensor([word_to_ix[word]])\\n      return self.embeddings(word).view(1,-1)      \\n\\n\\ncounter = 0\\nlosses = []\\n\\nEMBEDDING_DIM = 99\\nmodel = CBOW(vocab_size=len(vocab), embedding_dim=EMBEDDING_DIM)\\nloss_funtion = nn.NLLLoss()\\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\\n\\n\\nfor epoch in range(50):\\n  total_loss = 0\\n  #iterate over the rows in df -- note doing one at a time, not batches yet.\\n  for index, current_row in training_data.head(n=10000).iterrows():\\n    # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\\n    # into integer indices and wrap them in tensors)    \\n    context_vector = make_context_vector(current_row[\\'Context\\'])\\n    model.zero_grad()\\n    log_probs = model(context_vector)\\n    loss = loss_funtion(log_probs, torch.tensor([current_row[\\'Target\\']], dtype=torch.long))\\n    loss.backward()\\n    optimizer.step()\\n    # Get the Python number from a 1-element Tensor by calling tensor.item()\\n    total_loss += loss.item()\\n    counter += 1\\n    # if(counter % 1000 == 0):\\n    #   print(total_loss)\\n  print(f\"{epoch}:{loss.item()}\")\\n  losses.append(total_loss)\\n\\nprint(losses)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1WIXC1OsFcA",
        "colab_type": "code",
        "outputId": "33c9200a-ed3a-4955-e63a-7336cac213b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# preferred method: device agnostic tensor instantiation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxGCOWXLN4gY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW_Batch(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "      x_0\n",
        "      x_1                                     W_context                                     W_words\n",
        "            ==> sum(x0... x3)   * [embedding_dim, hidden_layer]  ==> Relu()  (H)   *   [hidden_layer, vocab_size]   ==> Softmax() \n",
        "              [1,embedding_dim]                                                                                       [vocab_size]\n",
        "      x_2\n",
        "      x_3    \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_layer = 128):\n",
        "      super(CBOW_Batch, self).__init__()\n",
        "      self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "      self.linear1 = nn.Linear(in_features=embedding_dim, out_features=hidden_layer)\n",
        "      self.linear2 = nn.Linear(in_features=hidden_layer, out_features=vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        inputs (torch.Tensor): An input data sensor inputs.shape = (batch, input_dim)\n",
        "      \"\"\"\n",
        "      in_shape = inputs.shape\n",
        "      #print(in_shape)\n",
        "      #https://stackoverflow.com/questions/44790670/torch-sum-a-tensor-along-an-axis\n",
        "      \"\"\"\n",
        "      The simplest and best solution is to use torch.sum().\n",
        "      To sum all elements of a tensor:\n",
        "        torch.sum(outputs) # gives back a scalar\n",
        "      To sum over all rows (i.e. for each column):\n",
        "        torch.sum(outputs, dim=0) # size = [1, ncol]\n",
        "      To sum over all columns (i.e. for each row):\n",
        "        torch.sum(outputs, dim=1) # size = [nrow, 1]\n",
        "\n",
        "      The only change i had to do to handle large batches is to use torch.sum with dim = 1, i.e. sum for each row = input.        \n",
        "      \"\"\"\n",
        "      x_embedded_sum = torch.sum(self.embeddings(inputs), dim=1).view((in_shape[0],-1))\n",
        "      \"\"\"\n",
        "      https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch\n",
        "      What is the meaning of parameter -1?\n",
        "      If there is any situation that you don't know how many rows you want but are sure of the number \n",
        "      of columns, then you can specify this with a -1. (Note that you can extend this to tensors with \n",
        "      more dimensions. Only one of the axis value can be -1). This is a way of telling the library: \n",
        "      \"give me a tensor that has these many columns and you compute the appropriate number of rows that \n",
        "      is necessary to make this happen\".      \n",
        "      \"\"\"\n",
        "      out1 = F.relu(self.linear1(x_embedded_sum))\n",
        "      out2 = self.linear2(out1)\n",
        "      log_probs = F.log_softmax(out2, dim=1)\n",
        "      \"\"\"\n",
        "      axis : axis along which we want to calculate the sum value. Otherwise, it will consider arr to be \n",
        "      flattened(works on all the axis). axis = 0 means along the column and axis = 1 means working along \n",
        "      the row.\n",
        "      \"\"\"\n",
        "      return log_probs\n",
        "\n",
        "    def get_word_emdedding(self, word):\n",
        "      word = torch.LongTensor([word_to_ix[word]])\n",
        "      return self.embeddings(word).view(1,-1)      \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATVXn73Cmc15",
        "colab_type": "code",
        "outputId": "046f5d98-761a-4f95-dfc6-cc1a6117d2c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\"\"\"\n",
        "Hi. I also had a hard time grasping this, especially because of confusion between CrossEntropyLoss and NLLLoss. Not sure if implementation of negative log likelihood loss was ever explained in courses. In short - CrossEntropyLoss = LogSoftmax + NLLLoss. Here is a quick example with NLLLoss implemenation:\n",
        "https://forums.fast.ai/t/nllloss-implementation/20028\n",
        "Quick videos:\n",
        "https://www.youtube.com/watch?v=tRsSi_sqXjI\n",
        "https://www.youtube.com/watch?v=bLb_Kp5Q9cw\n",
        "\"\"\"\n",
        "import torch\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def NLLLoss(logs, targets):\n",
        "    out = torch.zeros_like(targets, dtype=torch.float)\n",
        "    for i in range(len(targets)):\n",
        "        out[i] = logs[i][targets[i]]\n",
        "    return -out.sum()/len(out)\n",
        "\n",
        "x = torch.randn(3, 5)\n",
        "y = torch.LongTensor([0, 1, 2])\n",
        "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "x_log = log_softmax(x)\n",
        "\n",
        "nll_loss = torch.nn.NLLLoss()\n",
        "print(\"Torch CrossEntropyLoss: \", cross_entropy_loss(x, y))\n",
        "print(\"Torch NLL loss: \", nll_loss(x_log, y))\n",
        "print(\"Custom NLL loss: \", NLLLoss(x_log, y))\n",
        "# Torch CrossEntropyLoss:  tensor(1.8739)\n",
        "# Torch NLL loss:  tensor(1.8739)\n",
        "# Custom NLL loss:  tensor(1.8739)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch CrossEntropyLoss:  tensor(1.8739)\n",
            "Torch NLL loss:  tensor(1.8739)\n",
            "Custom NLL loss:  tensor(1.8739)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPA5duHah6iy",
        "colab_type": "code",
        "outputId": "dff73d4a-199c-463f-978b-d7c3aaa79d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import numpy as np\n",
        "training_data = cbow_data[cbow_data['split'] == 'train']\n",
        "#training_data = training_data.head(n=200000)\n",
        "print(f\"\\n Inputs = {training_data.count()}\")\n",
        "counter = 0\n",
        "losses = []\n",
        "#these are all hyper params, tried different values, 16, 32, 64....\n",
        "EMBEDDING_DIM = 64\n",
        "#the hidden layer doesnt seem to have much impact on the loss.\n",
        "#now trying number of dimensions\n",
        "model = CBOW_Batch(vocab_size=len(vocab), embedding_dim=EMBEDDING_DIM, hidden_layer = 64)\n",
        "loss_funtion = nn.NLLLoss()\n",
        "#increased learning rate from 0.001 to 0.005, much faster loss decrease.\n",
        "#ideally should use adam or adagrad or something.\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. Here we will use Adam; the optim package contains many other\n",
        "# optimization algoriths. The first argument to the Adam constructor tells the\n",
        "# optimizer which Tensors it should update.\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Inputs = Context    20977\n",
            "Target     20977\n",
            "split      20977\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjxscEh6Mop0",
        "colab_type": "code",
        "outputId": "a63a6ac2-1f48-44d0-93f1-b1f5cf4c5565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "DESIRED_BATCH_SIZE = 32\n",
        "NUM_OF_SPLITS = int(len(training_data)/DESIRED_BATCH_SIZE)\n",
        "PRINT = int(NUM_OF_SPLITS/10) #print 10 \".\" for every iteration\n",
        "#batch_size * NUM_OF_SPLITS = training data size \n",
        "\n",
        "print(f\"DESIRED_BATCH_SIZE = {DESIRED_BATCH_SIZE}; NUM_OF_SPLITS = {NUM_OF_SPLITS}, PRINT ={PRINT}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DESIRED_BATCH_SIZE = 32; NUM_OF_SPLITS = 655, PRINT =65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1TkBnZIevnj",
        "colab_type": "code",
        "outputId": "54627a98-30f2-4c2a-f84c-68c00182ebbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "struggled a lot with the tensor creation. at the end of the daty, pytorch tensors work best with \n",
        "numpy arrays. Now, we created the context vector as numpy arrays but were dealing wiht dataframes\n",
        "hence, i had to convert things back to numpy arrays and work with them.\n",
        "\"\"\"\n",
        "def make_context_vector_numpy(context_ids_numpy):\n",
        "    return torch.from_numpy(context_ids_numpy)\n",
        "\n",
        "for epoch in range(256):\n",
        "  total_loss = 0\n",
        "  counter = 0\n",
        "  for batch in np.array_split(training_data, NUM_OF_SPLITS):\n",
        "    #fill up a numpy array.\n",
        "    npArrayContext = np.full((batch.shape[0], 4), -1)\n",
        "    npArrayTgt = np.full(batch.shape[0], -1)\n",
        "    for index, current_row in batch.iterrows():\n",
        "      npArrayContext[index % batch.shape[0] ,:] = current_row['Context']\n",
        "      npArrayTgt[index % batch.shape[0]] = current_row['Target']\n",
        "\n",
        "    #print(f\"\\n npArray =>> {npArray}\")\n",
        "    #print(f\"\\n counter = [{counter}], npArray.shape = {npArray.shape} \\n npArray.dtype = {npArray.dtype}\")\n",
        "\n",
        "    #all these things were tried but didnt work.\n",
        "    #print(f\"\\n counter = [{counter}], batch.shape = {batch.shape} \\n batch['Context'].shape = {batch['Context'].shape} \\n batch['Context'].dtype = {batch['Context'].to_numpy().dtype}\")\n",
        "    #Av = np.hstack(A)\n",
        "    #print(f\"\\n batch['Context'] =>> {batch['Context']}\")\n",
        "    #print(f\"\\n batch['Context'].to_numpy() =>> {batch['Context'].to_numpy()}\")\n",
        "\n",
        "    # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "    # into integer indices and wrap them in tensors)    \n",
        "    context_vector =  make_context_vector_numpy(npArrayContext)\n",
        "    #print(f\"context_vector.shape = {context_vector.shape}\")\n",
        "\n",
        "    model.zero_grad()\n",
        "    log_probs = model(context_vector)\n",
        "    loss = loss_funtion(log_probs, torch.from_numpy(npArrayTgt))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "    # print(f\"\\n counter = [{counter}], loss.item() = {loss.item()}\")\n",
        "    # use the argument \"end\" to specify the end of line string and print with space\n",
        "    #print(f\".{batch.shape[0]}\", end = '')\n",
        "    if(counter % PRINT == 0):\n",
        "      print(f\".\", end = '')\n",
        "    total_loss += loss.item()\n",
        "    counter += 1\n",
        "    # if(counter % 1000 == 0):\n",
        "    #   print(total_loss)\n",
        "  print(f\"\\n[{epoch}]:[{counter}]: Loss = {total_loss}\")\n",
        "  losses.append(total_loss)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...........\n",
            "[0]:[655]: Loss = 5739.956849098206\n",
            "...........\n",
            "[1]:[655]: Loss = 5255.197099208832\n",
            "...........\n",
            "[2]:[655]: Loss = 4986.073739528656\n",
            "...........\n",
            "[3]:[655]: Loss = 4821.721682548523\n",
            "...........\n",
            "[4]:[655]: Loss = 4701.594984054565\n",
            "...........\n",
            "[5]:[655]: Loss = 4597.776047706604\n",
            "...........\n",
            "[6]:[655]: Loss = 4499.502867221832\n",
            "...........\n",
            "[7]:[655]: Loss = 4401.886965751648\n",
            "...........\n",
            "[8]:[655]: Loss = 4302.182397842407\n",
            "...........\n",
            "[9]:[655]: Loss = 4198.63724565506\n",
            "...........\n",
            "[10]:[655]: Loss = 4090.0838899612427\n",
            "...........\n",
            "[11]:[655]: Loss = 3975.897720813751\n",
            "...........\n",
            "[12]:[655]: Loss = 3855.9206318855286\n",
            "...........\n",
            "[13]:[655]: Loss = 3730.5613255500793\n",
            "...........\n",
            "[14]:[655]: Loss = 3600.7298860549927\n",
            "...........\n",
            "[15]:[655]: Loss = 3467.889811038971\n",
            "...........\n",
            "[16]:[655]: Loss = 3334.320272922516\n",
            "...........\n",
            "[17]:[655]: Loss = 3202.738653421402\n",
            "...........\n",
            "[18]:[655]: Loss = 3075.9734489917755\n",
            "...........\n",
            "[19]:[655]: Loss = 2956.779105901718\n",
            "...........\n",
            "[20]:[655]: Loss = 2846.8639874458313\n",
            "...........\n",
            "[21]:[655]: Loss = 2747.1121780872345\n",
            "...........\n",
            "[22]:[655]: Loss = 2657.3491978645325\n",
            "...........\n",
            "[23]:[655]: Loss = 2576.862882375717\n",
            "...........\n",
            "[24]:[655]: Loss = 2504.6778576374054\n",
            "...........\n",
            "[25]:[655]: Loss = 2439.6434540748596\n",
            "...........\n",
            "[26]:[655]: Loss = 2380.7528121471405\n",
            "...........\n",
            "[27]:[655]: Loss = 2327.0920927524567\n",
            "...........\n",
            "[28]:[655]: Loss = 2277.7649178504944\n",
            "...........\n",
            "[29]:[655]: Loss = 2232.201548218727\n",
            "...........\n",
            "[30]:[655]: Loss = 2189.696488380432\n",
            "...........\n",
            "[31]:[655]: Loss = 2149.9415471553802\n",
            "...........\n",
            "[32]:[655]: Loss = 2112.365389108658\n",
            "...........\n",
            "[33]:[655]: Loss = 2076.798861503601\n",
            "...........\n",
            "[34]:[655]: Loss = 2042.8781349658966\n",
            "...........\n",
            "[35]:[655]: Loss = 2010.3917359113693\n",
            "...........\n",
            "[36]:[655]: Loss = 1979.212688922882\n",
            "...........\n",
            "[37]:[655]: Loss = 1949.1880702972412\n",
            "...........\n",
            "[38]:[655]: Loss = 1920.0910111665726\n",
            "...........\n",
            "[39]:[655]: Loss = 1891.9238237142563\n",
            "...........\n",
            "[40]:[655]: Loss = 1864.5928556919098\n",
            "...........\n",
            "[41]:[655]: Loss = 1837.931304693222\n",
            "...........\n",
            "[42]:[655]: Loss = 1811.9696539640427\n",
            "...........\n",
            "[43]:[655]: Loss = 1786.6252074241638\n",
            "...........\n",
            "[44]:[655]: Loss = 1761.8014137744904\n",
            "...........\n",
            "[45]:[655]: Loss = 1737.5589761734009\n",
            "...........\n",
            "[46]:[655]: Loss = 1713.8146483898163\n",
            "...........\n",
            "[47]:[655]: Loss = 1690.4978272914886\n",
            "...........\n",
            "[48]:[655]: Loss = 1667.7006131410599\n",
            "...........\n",
            "[49]:[655]: Loss = 1645.3036959171295\n",
            "...........\n",
            "[50]:[655]: Loss = 1623.3110632896423\n",
            "...........\n",
            "[51]:[655]: Loss = 1601.720725774765\n",
            "...........\n",
            "[52]:[655]: Loss = 1580.484899878502\n",
            "...........\n",
            "[53]:[655]: Loss = 1559.5992839336395\n",
            "...........\n",
            "[54]:[655]: Loss = 1539.0417714118958\n",
            "...........\n",
            "[55]:[655]: Loss = 1518.791641831398\n",
            "...........\n",
            "[56]:[655]: Loss = 1498.8739395141602\n",
            "...........\n",
            "[57]:[655]: Loss = 1479.2018854618073\n",
            "...........\n",
            "[58]:[655]: Loss = 1459.8497797250748\n",
            "...........\n",
            "[59]:[655]: Loss = 1440.7527315616608\n",
            "...........\n",
            "[60]:[655]: Loss = 1421.9350471496582\n",
            "...........\n",
            "[61]:[655]: Loss = 1403.381408572197\n",
            "...........\n",
            "[62]:[655]: Loss = 1385.0480533838272\n",
            "...........\n",
            "[63]:[655]: Loss = 1366.9774532318115\n",
            "...........\n",
            "[64]:[655]: Loss = 1349.1313471794128\n",
            "...........\n",
            "[65]:[655]: Loss = 1331.5021364688873\n",
            "...........\n",
            "[66]:[655]: Loss = 1314.0917938947678\n",
            "...........\n",
            "[67]:[655]: Loss = 1296.9074743390083\n",
            "...........\n",
            "[68]:[655]: Loss = 1279.9184927940369\n",
            "...........\n",
            "[69]:[655]: Loss = 1263.1077153086662\n",
            "...........\n",
            "[70]:[655]: Loss = 1246.5012026429176\n",
            "...........\n",
            "[71]:[655]: Loss = 1230.0848610401154\n",
            "...........\n",
            "[72]:[655]: Loss = 1213.8465604186058\n",
            "...........\n",
            "[73]:[655]: Loss = 1197.8203146457672\n",
            "...........\n",
            "[74]:[655]: Loss = 1181.9236288070679\n",
            "...........\n",
            "[75]:[655]: Loss = 1166.2501864433289\n",
            "...........\n",
            "[76]:[655]: Loss = 1150.6987961530685\n",
            "...........\n",
            "[77]:[655]: Loss = 1135.2859721779823\n",
            "...........\n",
            "[78]:[655]: Loss = 1120.0744670033455\n",
            "...........\n",
            "[79]:[655]: Loss = 1104.9864109754562\n",
            "...........\n",
            "[80]:[655]: Loss = 1090.0493885874748\n",
            "...........\n",
            "[81]:[655]: Loss = 1075.26962774992\n",
            "...........\n",
            "[82]:[655]: Loss = 1060.6537697315216\n",
            "...........\n",
            "[83]:[655]: Loss = 1046.1402158141136\n",
            "...........\n",
            "[84]:[655]: Loss = 1031.7624379992485\n",
            "...........\n",
            "[85]:[655]: Loss = 1017.5628089308739\n",
            "...........\n",
            "[86]:[655]: Loss = 1003.4557093977928\n",
            "...........\n",
            "[87]:[655]: Loss = 989.5024774670601\n",
            "...........\n",
            "[88]:[655]: Loss = 975.6686109900475\n",
            "...........\n",
            "[89]:[655]: Loss = 961.971630692482\n",
            "...........\n",
            "[90]:[655]: Loss = 948.3712622523308\n",
            "...........\n",
            "[91]:[655]: Loss = 934.8973999619484\n",
            "...........\n",
            "[92]:[655]: Loss = 921.5286129713058\n",
            "...........\n",
            "[93]:[655]: Loss = 908.3010120391846\n",
            "...........\n",
            "[94]:[655]: Loss = 895.1708254218102\n",
            "...........\n",
            "[95]:[655]: Loss = 882.1565333604813\n",
            "...........\n",
            "[96]:[655]: Loss = 869.2882147431374\n",
            "...........\n",
            "[97]:[655]: Loss = 856.4978106021881\n",
            "...........\n",
            "[98]:[655]: Loss = 843.8335806727409\n",
            "...........\n",
            "[99]:[655]: Loss = 831.2535967230797\n",
            "...........\n",
            "[100]:[655]: Loss = 818.812005341053\n",
            "...........\n",
            "[101]:[655]: Loss = 806.4414900541306\n",
            "...........\n",
            "[102]:[655]: Loss = 794.2207453846931\n",
            "...........\n",
            "[103]:[655]: Loss = 782.0791589021683\n",
            "...........\n",
            "[104]:[655]: Loss = 770.0503720343113\n",
            "...........\n",
            "[105]:[655]: Loss = 758.0937044620514\n",
            "...........\n",
            "[106]:[655]: Loss = 746.254734903574\n",
            "...........\n",
            "[107]:[655]: Loss = 734.5147829055786\n",
            "...........\n",
            "[108]:[655]: Loss = 722.8708672821522\n",
            "...........\n",
            "[109]:[655]: Loss = 711.312854796648\n",
            "...........\n",
            "[110]:[655]: Loss = 699.8898876905441\n",
            "...........\n",
            "[111]:[655]: Loss = 688.5479443371296\n",
            "...........\n",
            "[112]:[655]: Loss = 677.3170328736305\n",
            "...........\n",
            "[113]:[655]: Loss = 666.1657552719116\n",
            "...........\n",
            "[114]:[655]: Loss = 655.1052687466145\n",
            "...........\n",
            "[115]:[655]: Loss = 644.1574934124947\n",
            "...........\n",
            "[116]:[655]: Loss = 633.294456422329\n",
            "...........\n",
            "[117]:[655]: Loss = 622.5723321139812\n",
            "...........\n",
            "[118]:[655]: Loss = 611.8825574815273\n",
            "...........\n",
            "[119]:[655]: Loss = 601.3246859312057\n",
            "...........\n",
            "[120]:[655]: Loss = 590.8796343803406\n",
            "...........\n",
            "[121]:[655]: Loss = 580.5109828710556\n",
            "...........\n",
            "[122]:[655]: Loss = 570.2342020869255\n",
            "...........\n",
            "[123]:[655]: Loss = 560.0683705210686\n",
            "...........\n",
            "[124]:[655]: Loss = 549.9810843765736\n",
            "...........\n",
            "[125]:[655]: Loss = 539.9968796372414\n",
            "...........\n",
            "[126]:[655]: Loss = 530.0935475230217\n",
            "...........\n",
            "[127]:[655]: Loss = 520.2689153552055\n",
            "...........\n",
            "[128]:[655]: Loss = 510.56475284695625\n",
            "...........\n",
            "[129]:[655]: Loss = 500.94077521562576\n",
            "...........\n",
            "[130]:[655]: Loss = 491.4430200755596\n",
            "...........\n",
            "[131]:[655]: Loss = 481.98947367072105\n",
            "...........\n",
            "[132]:[655]: Loss = 472.6548262834549\n",
            "...........\n",
            "[133]:[655]: Loss = 463.4169189184904\n",
            "...........\n",
            "[134]:[655]: Loss = 454.2762026935816\n",
            "...........\n",
            "[135]:[655]: Loss = 445.22178588807583\n",
            "...........\n",
            "[136]:[655]: Loss = 436.275328412652\n",
            "...........\n",
            "[137]:[655]: Loss = 427.4176718890667\n",
            "...........\n",
            "[138]:[655]: Loss = 418.6536106914282\n",
            "...........\n",
            "[139]:[655]: Loss = 410.0012225061655\n",
            "...........\n",
            "[140]:[655]: Loss = 401.43567080795765\n",
            "...........\n",
            "[141]:[655]: Loss = 392.9800883680582\n",
            "...........\n",
            "[142]:[655]: Loss = 384.61853525042534\n",
            "...........\n",
            "[143]:[655]: Loss = 376.3266156613827\n",
            "...........\n",
            "[144]:[655]: Loss = 368.1604046225548\n",
            "...........\n",
            "[145]:[655]: Loss = 360.0803081840277\n",
            "...........\n",
            "[146]:[655]: Loss = 352.1010009944439\n",
            "...........\n",
            "[147]:[655]: Loss = 344.2097085118294\n",
            "...........\n",
            "[148]:[655]: Loss = 336.4043805897236\n",
            "...........\n",
            "[149]:[655]: Loss = 328.70229928195477\n",
            "...........\n",
            "[150]:[655]: Loss = 321.102886274457\n",
            "...........\n",
            "[151]:[655]: Loss = 313.5816203504801\n",
            "...........\n",
            "[152]:[655]: Loss = 306.14998023211956\n",
            "...........\n",
            "[153]:[655]: Loss = 298.8052793890238\n",
            "...........\n",
            "[154]:[655]: Loss = 291.5769903883338\n",
            "...........\n",
            "[155]:[655]: Loss = 284.42195227742195\n",
            "...........\n",
            "[156]:[655]: Loss = 277.3694043830037\n",
            "...........\n",
            "[157]:[655]: Loss = 270.4221298545599\n",
            "...........\n",
            "[158]:[655]: Loss = 263.5489344894886\n",
            "...........\n",
            "[159]:[655]: Loss = 256.78879168629646\n",
            "...........\n",
            "[160]:[655]: Loss = 250.09357479959726\n",
            "...........\n",
            "[161]:[655]: Loss = 243.5129312351346\n",
            "...........\n",
            "[162]:[655]: Loss = 237.00840974599123\n",
            "...........\n",
            "[163]:[655]: Loss = 230.6228074207902\n",
            "...........\n",
            "[164]:[655]: Loss = 224.32005345076323\n",
            "...........\n",
            "[165]:[655]: Loss = 218.1312498152256\n",
            "...........\n",
            "[166]:[655]: Loss = 211.99764093011618\n",
            "...........\n",
            "[167]:[655]: Loss = 205.97829047590494\n",
            "...........\n",
            "[168]:[655]: Loss = 200.03971147164702\n",
            "...........\n",
            "[169]:[655]: Loss = 194.21498948335648\n",
            "...........\n",
            "[170]:[655]: Loss = 188.47598014771938\n",
            "...........\n",
            "[171]:[655]: Loss = 182.81717463955283\n",
            "...........\n",
            "[172]:[655]: Loss = 177.2798410654068\n",
            "...........\n",
            "[173]:[655]: Loss = 171.82206044346094\n",
            "...........\n",
            "[174]:[655]: Loss = 166.4673763923347\n",
            "...........\n",
            "[175]:[655]: Loss = 161.20578694716096\n",
            "...........\n",
            "[176]:[655]: Loss = 156.04740618541837\n",
            "...........\n",
            "[177]:[655]: Loss = 150.968962572515\n",
            "...........\n",
            "[178]:[655]: Loss = 145.9895248375833\n",
            "...........\n",
            "[179]:[655]: Loss = 141.114750508219\n",
            "...........\n",
            "[180]:[655]: Loss = 136.32245467975736\n",
            "...........\n",
            "[181]:[655]: Loss = 131.63578648492694\n",
            "...........\n",
            "[182]:[655]: Loss = 127.03329007513821\n",
            "...........\n",
            "[183]:[655]: Loss = 122.53960422053933\n",
            "...........\n",
            "[184]:[655]: Loss = 118.13470958732069\n",
            "...........\n",
            "[185]:[655]: Loss = 113.83284168876708\n",
            "...........\n",
            "[186]:[655]: Loss = 109.63363529369235\n",
            "...........\n",
            "[187]:[655]: Loss = 105.50993129424751\n",
            "...........\n",
            "[188]:[655]: Loss = 101.48562987335026\n",
            "...........\n",
            "[189]:[655]: Loss = 97.55703434348106\n",
            "...........\n",
            "[190]:[655]: Loss = 93.73209620267153\n",
            "...........\n",
            "[191]:[655]: Loss = 89.99172875471413\n",
            "...........\n",
            "[192]:[655]: Loss = 86.33507675118744\n",
            "...........\n",
            "[193]:[655]: Loss = 82.7852587942034\n",
            "...........\n",
            "[194]:[655]: Loss = 79.32849569246173\n",
            "...........\n",
            "[195]:[655]: Loss = 75.96235965564847\n",
            "...........\n",
            "[196]:[655]: Loss = 72.68760440964252\n",
            "...........\n",
            "[197]:[655]: Loss = 69.50273846462369\n",
            "...........\n",
            "[198]:[655]: Loss = 66.4019173020497\n",
            "...........\n",
            "[199]:[655]: Loss = 63.39990985393524\n",
            "...........\n",
            "[200]:[655]: Loss = 60.479737675748765\n",
            "...........\n",
            "[201]:[655]: Loss = 57.64366481732577\n",
            "...........\n",
            "[202]:[655]: Loss = 54.901850833557546\n",
            "...........\n",
            "[203]:[655]: Loss = 52.247359725646675\n",
            "...........\n",
            "[204]:[655]: Loss = 49.67559851612896\n",
            "...........\n",
            "[205]:[655]: Loss = 47.18007269781083\n",
            "...........\n",
            "[206]:[655]: Loss = 44.77551549859345\n",
            "...........\n",
            "[207]:[655]: Loss = 42.450182197615504\n",
            "...........\n",
            "[208]:[655]: Loss = 40.20811188779771\n",
            "...........\n",
            "[209]:[655]: Loss = 38.0480289706029\n",
            "...........\n",
            "[210]:[655]: Loss = 35.97243811190128\n",
            "...........\n",
            "[211]:[655]: Loss = 33.97384871309623\n",
            "...........\n",
            "[212]:[655]: Loss = 32.0558792850934\n",
            "...........\n",
            "[213]:[655]: Loss = 30.218219832517207\n",
            "...........\n",
            "[214]:[655]: Loss = 28.45254745753482\n",
            "...........\n",
            "[215]:[655]: Loss = 26.765892839524895\n",
            "...........\n",
            "[216]:[655]: Loss = 25.15280205057934\n",
            "...........\n",
            "[217]:[655]: Loss = 23.609716138336807\n",
            "...........\n",
            "[218]:[655]: Loss = 22.136998364701867\n",
            "...........\n",
            "[219]:[655]: Loss = 20.73532212153077\n",
            "...........\n",
            "[220]:[655]: Loss = 19.40355317806825\n",
            "...........\n",
            "[221]:[655]: Loss = 18.134188441559672\n",
            "...........\n",
            "[222]:[655]: Loss = 16.93416436831467\n",
            "...........\n",
            "[223]:[655]: Loss = 15.792279965244234\n",
            "...........\n",
            "[224]:[655]: Loss = 14.711072890087962\n",
            "...........\n",
            "[225]:[655]: Loss = 13.694278823910281\n",
            "...........\n",
            "[226]:[655]: Loss = 12.727603655541316\n",
            "...........\n",
            "[227]:[655]: Loss = 11.820655342657119\n",
            "...........\n",
            "[228]:[655]: Loss = 10.963797977892682\n",
            "...........\n",
            "[229]:[655]: Loss = 10.158269508508965\n",
            "...........\n",
            "[230]:[655]: Loss = 9.40396857727319\n",
            "...........\n",
            "[231]:[655]: Loss = 8.69590427633375\n",
            "...........\n",
            "[232]:[655]: Loss = 8.037310283631086\n",
            "...........\n",
            "[233]:[655]: Loss = 7.419206298189238\n",
            "...........\n",
            "[234]:[655]: Loss = 6.851973547250964\n",
            "...........\n",
            "[235]:[655]: Loss = 6.315805891295895\n",
            "...........\n",
            "[236]:[655]: Loss = 5.822982613462955\n",
            "...........\n",
            "[237]:[655]: Loss = 5.366573510575108\n",
            "...........\n",
            "[238]:[655]: Loss = 4.946435317979194\n",
            "...........\n",
            "[239]:[655]: Loss = 4.556659938883968\n",
            "...........\n",
            "[240]:[655]: Loss = 4.199208817561157\n",
            "...........\n",
            "[241]:[655]: Loss = 3.8716999860480428\n",
            "...........\n",
            "[242]:[655]: Loss = 3.5670536215766333\n",
            "...........\n",
            "[243]:[655]: Loss = 3.2890193163766526\n",
            "...........\n",
            "[244]:[655]: Loss = 3.03300193924224\n",
            "...........\n",
            "[245]:[655]: Loss = 2.8035089453333057\n",
            "...........\n",
            "[246]:[655]: Loss = 2.5863592730020173\n",
            "...........\n",
            "[247]:[655]: Loss = 2.388891948387027\n",
            "...........\n",
            "[248]:[655]: Loss = 2.2092433103825897\n",
            "...........\n",
            "[249]:[655]: Loss = 2.0444439741258975\n",
            "...........\n",
            "[250]:[655]: Loss = 1.8936896714440081\n",
            "...........\n",
            "[251]:[655]: Loss = 1.7557260945905\n",
            "...........\n",
            "[252]:[655]: Loss = 1.6308709980512504\n",
            "...........\n",
            "[253]:[655]: Loss = 1.5155196012929082\n",
            "...........\n",
            "[254]:[655]: Loss = 1.4110867130511906\n",
            "...........\n",
            "[255]:[655]: Loss = 1.3147707747120876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQB7cwpthQoI",
        "colab_type": "code",
        "outputId": "978883c4-e382-4750-88ab-039e3dc2bfad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(losses)\n",
        "\n",
        "# with open('/content/gdrive/My Drive/filename.txt', 'w') as f:\n",
        "#    f.write('values')\n",
        "\n",
        "#/content/drive/My Drive/Public Shared/Project-2-Word-Embeddings/   \n",
        "torch.save(model.state_dict(), \"cbow-model-frankenstein.bin\")\n",
        "\n",
        "!cp \"cbow-model-frankenstein.bin\" \"/content/drive/My Drive/Public Shared/Project-2-Word-Embeddings/cbow-model-frankenstein.bin\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5739.956849098206, 5255.197099208832, 4986.073739528656, 4821.721682548523, 4701.594984054565, 4597.776047706604, 4499.502867221832, 4401.886965751648, 4302.182397842407, 4198.63724565506, 4090.0838899612427, 3975.897720813751, 3855.9206318855286, 3730.5613255500793, 3600.7298860549927, 3467.889811038971, 3334.320272922516, 3202.738653421402, 3075.9734489917755, 2956.779105901718, 2846.8639874458313, 2747.1121780872345, 2657.3491978645325, 2576.862882375717, 2504.6778576374054, 2439.6434540748596, 2380.7528121471405, 2327.0920927524567, 2277.7649178504944, 2232.201548218727, 2189.696488380432, 2149.9415471553802, 2112.365389108658, 2076.798861503601, 2042.8781349658966, 2010.3917359113693, 1979.212688922882, 1949.1880702972412, 1920.0910111665726, 1891.9238237142563, 1864.5928556919098, 1837.931304693222, 1811.9696539640427, 1786.6252074241638, 1761.8014137744904, 1737.5589761734009, 1713.8146483898163, 1690.4978272914886, 1667.7006131410599, 1645.3036959171295, 1623.3110632896423, 1601.720725774765, 1580.484899878502, 1559.5992839336395, 1539.0417714118958, 1518.791641831398, 1498.8739395141602, 1479.2018854618073, 1459.8497797250748, 1440.7527315616608, 1421.9350471496582, 1403.381408572197, 1385.0480533838272, 1366.9774532318115, 1349.1313471794128, 1331.5021364688873, 1314.0917938947678, 1296.9074743390083, 1279.9184927940369, 1263.1077153086662, 1246.5012026429176, 1230.0848610401154, 1213.8465604186058, 1197.8203146457672, 1181.9236288070679, 1166.2501864433289, 1150.6987961530685, 1135.2859721779823, 1120.0744670033455, 1104.9864109754562, 1090.0493885874748, 1075.26962774992, 1060.6537697315216, 1046.1402158141136, 1031.7624379992485, 1017.5628089308739, 1003.4557093977928, 989.5024774670601, 975.6686109900475, 961.971630692482, 948.3712622523308, 934.8973999619484, 921.5286129713058, 908.3010120391846, 895.1708254218102, 882.1565333604813, 869.2882147431374, 856.4978106021881, 843.8335806727409, 831.2535967230797, 818.812005341053, 806.4414900541306, 794.2207453846931, 782.0791589021683, 770.0503720343113, 758.0937044620514, 746.254734903574, 734.5147829055786, 722.8708672821522, 711.312854796648, 699.8898876905441, 688.5479443371296, 677.3170328736305, 666.1657552719116, 655.1052687466145, 644.1574934124947, 633.294456422329, 622.5723321139812, 611.8825574815273, 601.3246859312057, 590.8796343803406, 580.5109828710556, 570.2342020869255, 560.0683705210686, 549.9810843765736, 539.9968796372414, 530.0935475230217, 520.2689153552055, 510.56475284695625, 500.94077521562576, 491.4430200755596, 481.98947367072105, 472.6548262834549, 463.4169189184904, 454.2762026935816, 445.22178588807583, 436.275328412652, 427.4176718890667, 418.6536106914282, 410.0012225061655, 401.43567080795765, 392.9800883680582, 384.61853525042534, 376.3266156613827, 368.1604046225548, 360.0803081840277, 352.1010009944439, 344.2097085118294, 336.4043805897236, 328.70229928195477, 321.102886274457, 313.5816203504801, 306.14998023211956, 298.8052793890238, 291.5769903883338, 284.42195227742195, 277.3694043830037, 270.4221298545599, 263.5489344894886, 256.78879168629646, 250.09357479959726, 243.5129312351346, 237.00840974599123, 230.6228074207902, 224.32005345076323, 218.1312498152256, 211.99764093011618, 205.97829047590494, 200.03971147164702, 194.21498948335648, 188.47598014771938, 182.81717463955283, 177.2798410654068, 171.82206044346094, 166.4673763923347, 161.20578694716096, 156.04740618541837, 150.968962572515, 145.9895248375833, 141.114750508219, 136.32245467975736, 131.63578648492694, 127.03329007513821, 122.53960422053933, 118.13470958732069, 113.83284168876708, 109.63363529369235, 105.50993129424751, 101.48562987335026, 97.55703434348106, 93.73209620267153, 89.99172875471413, 86.33507675118744, 82.7852587942034, 79.32849569246173, 75.96235965564847, 72.68760440964252, 69.50273846462369, 66.4019173020497, 63.39990985393524, 60.479737675748765, 57.64366481732577, 54.901850833557546, 52.247359725646675, 49.67559851612896, 47.18007269781083, 44.77551549859345, 42.450182197615504, 40.20811188779771, 38.0480289706029, 35.97243811190128, 33.97384871309623, 32.0558792850934, 30.218219832517207, 28.45254745753482, 26.765892839524895, 25.15280205057934, 23.609716138336807, 22.136998364701867, 20.73532212153077, 19.40355317806825, 18.134188441559672, 16.93416436831467, 15.792279965244234, 14.711072890087962, 13.694278823910281, 12.727603655541316, 11.820655342657119, 10.963797977892682, 10.158269508508965, 9.40396857727319, 8.69590427633375, 8.037310283631086, 7.419206298189238, 6.851973547250964, 6.315805891295895, 5.822982613462955, 5.366573510575108, 4.946435317979194, 4.556659938883968, 4.199208817561157, 3.8716999860480428, 3.5670536215766333, 3.2890193163766526, 3.03300193924224, 2.8035089453333057, 2.5863592730020173, 2.388891948387027, 2.2092433103825897, 2.0444439741258975, 1.8936896714440081, 1.7557260945905, 1.6308709980512504, 1.5155196012929082, 1.4110867130511906, 1.3147707747120876]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STN5KThvbb5I",
        "colab_type": "code",
        "outputId": "f1cac4b5-15d1-4222-b5b0-13e84d156fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "def print_words(context, actual_id, predicted_id):\n",
        "  #print(context)\n",
        "  for contextId in context:\n",
        "    print(ix_to_word[contextId], end=' ')\n",
        "  print(f\"; actual = {ix_to_word[actual_id]} , --  predicted = {ix_to_word[predicted_id]}\")\n",
        "\n",
        "#Word prediction\n",
        "#given a model, tell the most likely word.\n",
        "test_data = cbow_data[cbow_data['split'] == 'test']\n",
        "for index, current_row in training_data.head(n=50).iterrows():\n",
        "  npArrayContext = np.full((1, 4), -1)\n",
        "  npArrayTgt = np.full(1, -1)\n",
        "  npArrayContext[0 ,:] = current_row['Context']\n",
        "  npArrayTgt[0] = current_row['Target']\n",
        "  context_vector =  make_context_vector_numpy(npArrayContext)\n",
        "\n",
        "  #print(context_vector.shape)\n",
        "  log_probs = model(context_vector) #forward\n",
        "  indices = log_probs.argmax(1)\n",
        "  #print predicted and actual.\n",
        "  print_words(current_row['Context'], current_row['Target'], indices.numpy()[0])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "governments religions nations earth ; actual = different , --  predicted = different\n",
            "confessed guilty ascribed justine ; actual = crime , --  predicted = crime\n",
            "melancholy abode evidence easily ; actual = doubtless , --  predicted = doubtless\n",
            "involuntarily endeavoured duties regard ; actual = recollect , --  predicted = recollect\n",
            "one wandring pollutes day ; actual = thought , --  predicted = thought\n",
            "occupied attending subsistence decreased ; actual = means , --  predicted = means\n",
            "occupied arranging old man ; actual = cottage , --  predicted = cottage\n",
            "whole wretched justice suffered ; actual = mockery , --  predicted = mockery\n",
            "walton letter th march ; actual = archangel , --  predicted = archangel\n",
            "picture present human must ; actual = peaceful , --  predicted = peaceful\n",
            "could precipitate icerifts destroy ; actual = one , --  predicted = one\n",
            "bear live oblivion country ; actual = poverty , --  predicted = poverty\n",
            "watching wan friendhis eyes ; actual = countenance , --  predicted = countenance\n",
            "project gutenberg frankenstein file ; actual = ebook , --  predicted = ebook\n",
            "prophetic feeling heart sink ; actual = felt , --  predicted = felt\n",
            "rather ever discovered much ; actual = ignorant , --  predicted = ignorant\n",
            "morning distant leagues paris ; actual = many , --  predicted = many\n",
            "may absent two interfere ; actual = month , --  predicted = month\n",
            "cold less frame stature ; actual = injury , --  predicted = injury\n",
            "admire love past ages ; actual = heroes , --  predicted = heroes\n",
            "accustomed light objects right ; actual = perceive , --  predicted = perceive\n",
            "call forth sedulous attention ; actual = womans , --  predicted = womans\n",
            "aunt died one much ; actual = every , --  predicted = every\n",
            "dream vanished horror disgust ; actual = breathless , --  predicted = breathless\n",
            "soul may intellectual eye ; actual = fix , --  predicted = fix\n",
            "latter soon undivided attention ; actual = obtained , --  predicted = obtained\n",
            "many times satan fitter ; actual = considered , --  predicted = considered\n",
            "small price acquirement knowledge ; actual = pay , --  predicted = pay\n",
            "abortive creation greatest disdain ; actual = entertained , --  predicted = entertained\n",
            "double existence suffer misery ; actual = may , --  predicted = may\n",
            "although voice soft music ; actual = unlike , --  predicted = unlike\n",
            "bread cheese wine latter ; actual = milk , --  predicted = milk\n",
            "temper avoid attach fervently ; actual = crowd , --  predicted = crowd\n",
            "arrives grief indulgence necessity ; actual = rather , --  predicted = rather\n",
            "generally melancholy sometimes gnashes ; actual = despairing , --  predicted = despairing\n",
            "hut ran fields speed ; actual = across , --  predicted = across\n",
            "bent care distributing scanty ; actual = labour , --  predicted = labour\n",
            "morning rain dismally panes ; actual = pattered , --  predicted = pattered\n",
            "creation entertained disdain wouldbe ; actual = greatest , --  predicted = greatest\n",
            "upon modern terms shall ; actual = chemistry , --  predicted = chemistry\n",
            "courses rivers generally avoided ; actual = daemon , --  predicted = daemon\n",
            "busy one vessel apparently ; actual = side , --  predicted = side\n",
            "marks writing trees cut ; actual = barks , --  predicted = barks\n",
            "never reach forbear recording ; actual = yet , --  predicted = yet\n",
            "work copied anyone united ; actual = distributed , --  predicted = distributed\n",
            "project gutenbergtm party distributing ; actual = trademark , --  predicted = trademark\n",
            "indeed dream still alive ; actual = sorry , --  predicted = sorry\n",
            "ground pursuit much first ; actual = gained , --  predicted = gained\n",
            "short time enjoying transitory ; actual = shore , --  predicted = shore\n",
            "hung coffin concealed long ; actual = face , --  predicted = face\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdIbKLLFhmVb",
        "colab_type": "code",
        "outputId": "e0b0c254-536a-40c7-a81e-eab3c3143cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "#1. closest words\n",
        "def pretty_print(results):\n",
        "    \"\"\"\n",
        "    Pretty print embedding results.\n",
        "    \"\"\"\n",
        "    for item in results:\n",
        "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
        "\n",
        "def get_closest(target_word, fn_word_to_idx, embeddings, n=5):\n",
        "    \"\"\"\n",
        "    Get the n closest\n",
        "    words to your word.\n",
        "    \"\"\"\n",
        "    # Calculate distances to all other words\n",
        "    target_word_embedding = embeddings[fn_word_to_idx[target_word.lower()]]\n",
        "    distances = []\n",
        "    for word, index in fn_word_to_idx.items():\n",
        "        if word == target_word:\n",
        "            continue\n",
        "        distances.append((word, torch.dist(target_word_embedding, embeddings[index])))\n",
        "    \n",
        "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
        "    return results\n",
        "\n",
        "\n",
        "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
        "\n",
        "embeddings =  model.embeddings.weight.data\n",
        "\n",
        "for target_word in target_words: \n",
        "    print(f\"======={target_word}=======\")\n",
        "    if target_word not in word_to_ix:\n",
        "        print(\"Not in vocabulary\")\n",
        "        continue\n",
        "    pretty_print(get_closest(target_word, word_to_ix, embeddings, n=5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=======frankenstein=======\n",
            "...[8.28] - coward\n",
            "...[8.60] - songs\n",
            "...[8.66] - editions\n",
            "...[8.68] - impenetrable\n",
            "...[8.77] - inevitable\n",
            "...[8.82] - adieu\n",
            "=======monster=======\n",
            "...[7.94] - ephemeral\n",
            "...[8.02] - considering\n",
            "...[8.04] - curiosities\n",
            "...[8.23] - achieve\n",
            "...[8.27] - elevating\n",
            "...[8.35] - dissipate\n",
            "=======science=======\n",
            "...[10.00] - castles\n",
            "...[10.04] - belong\n",
            "...[10.07] - everywhere\n",
            "...[10.09] - pretence\n",
            "...[10.16] - beheld\n",
            "...[10.18] - incalculable\n",
            "=======sickness=======\n",
            "...[8.48] - introduced\n",
            "...[8.56] - gloried\n",
            "...[8.62] - lifemy\n",
            "...[8.68] - wrote\n",
            "...[8.77] - insufficient\n",
            "...[8.79] - marked\n",
            "=======lonely=======\n",
            "...[8.84] - community\n",
            "...[8.85] - elevating\n",
            "...[8.96] - archfiend\n",
            "...[8.97] - interrupted\n",
            "...[9.17] - go\n",
            "...[9.21] - conductors\n",
            "=======happy=======\n",
            "...[8.77] - devoured\n",
            "...[8.78] - condemns\n",
            "...[8.79] - enticing\n",
            "...[8.79] - classes\n",
            "...[8.81] - ability\n",
            "...[8.91] - hero\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as9cfc6qL-HQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2. word analogy\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}