{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2-word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "12HdrCGi4eDCcI0qf8xRhyb1mxLkNrVhD",
      "authorship_tag": "ABX9TyMsM9s+L7RNj166427NOGK1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talktovishal/NLP-ML-Projects/blob/master/Project2_word2vec-Part1-DownloadPreprocessKaggleData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P3n1AKc08Dp",
        "colab_type": "code",
        "outputId": "a90dd473-d64f-4357-eaa5-b2109aef1ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install kaggle\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.38.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEQbNFfy3mqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /root/.kaggle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KitUzNZD4LcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to-do  = remove the key before saving it to github\n",
        "!echo '{\"username\":\"thevishalchowdhary\",\"key\":\"XXXXXXXX\"}' > /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdY9vZAs4eec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuBNNfql41F-",
        "colab_type": "code",
        "outputId": "484b0b89-7151-4c58-8656-c3f7539aa600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "!kaggle datasets list"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                                         title                                                size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  \n",
            "unanimad/dataisbeautiful                                    Reddit - Data is Beautiful                           11MB  2020-03-21 22:28:28            794  \n",
            "allen-institute-for-ai/CORD-19-research-challenge           COVID-19 Open Research Dataset Challenge (CORD-19)  646MB  2020-03-20 23:31:34          28735  \n",
            "rubenssjr/brasilian-houses-to-rent                          brazilian_houses_to_rent                            117KB  2020-03-15 01:12:22            750  \n",
            "sudalairajkumar/novel-corona-virus-2019-dataset             Novel Corona Virus 2019 Dataset                     412KB  2020-03-24 06:10:13          79121  \n",
            "kimjihoo/coronavirusdataset                                 Data Science for COVID-19 (DS4C)                      3MB  2020-03-22 04:17:54          20489  \n",
            "jessemostipak/hotel-booking-demand                          Hotel booking demand                                  1MB  2020-02-13 01:27:20          10353  \n",
            "shivamb/real-or-fake-fake-jobposting-prediction             [Real or Fake] Fake JobPosting Prediction            16MB  2020-02-29 08:23:34           2104  \n",
            "brunotly/foreign-exchange-rates-per-dollar-20002019         Foreign Exchange Rates 2000-2019                      1MB  2020-03-03 17:43:07           1653  \n",
            "timoboz/data-science-cheat-sheets                           Data Science Cheat Sheets                           596MB  2020-02-04 19:42:27           6358  \n",
            "imdevskp/sars-outbreak-2003-complete-dataset                SARS 2003 Outbreak Complete Dataset                  10KB  2020-02-26 10:25:22           2022  \n",
            "imdevskp/ebola-outbreak-20142016-complete-dataset           Ebola 2014-2016 Outbreak Complete Dataset           101KB  2020-02-26 14:36:31           1922  \n",
            "tunguz/big-five-personality-test                            Big Five Personality Test                           159MB  2020-02-17 15:59:37           3157  \n",
            "paultimothymooney/coronavirus-genome-sequence               Coronavirus Genome Sequence                           9MB  2020-02-29 00:25:13            596  \n",
            "arindam235/startup-investments-crunchbase                   StartUp Investments (Crunchbase)                      3MB  2020-02-17 21:54:42           2207  \n",
            "timoboz/tesla-stock-data-from-2010-to-2020                  Tesla stock data from 2010 to 2020                   46KB  2020-02-04 17:15:32           3033  \n",
            "timoboz/python-data-science-handbook                        Python Data Science Handbook                         15MB  2020-02-04 18:27:14           1481  \n",
            "prakrutchauhan/indian-candidates-for-general-election-2019  Indian Candidates for General Election 2019         133KB  2020-03-03 07:01:53            992  \n",
            "gpiosenka/100-bird-species                                  160 Bird Species                                      2GB  2020-03-23 20:51:22           1033  \n",
            "ronitf/heart-disease-uci                                    Heart Disease UCI                                     3KB  2018-06-25 11:33:56         122157  \n",
            "gregorut/videogamesales                                     Video Game Sales                                    381KB  2016-10-26 09:10:49          93402  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMvGYrT55pEz",
        "colab_type": "code",
        "outputId": "a334770a-8ddc-4b96-ffe9-2ef07c7f9170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!kaggle datasets download snapcrack/all-the-news"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading all-the-news.zip to /content\n",
            " 89% 217M/244M [00:06<00:01, 19.1MB/s]\n",
            "100% 244M/244M [00:06<00:00, 41.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzKkqm9G5422",
        "colab_type": "code",
        "outputId": "0ae10e07-363b-4dee-e585-01cd3d74400d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!os.chdir('/content')\n",
        "for file in os.listdir():\n",
        "    print(file)\n",
        "    if(file.endswith('.zip')):\n",
        "      zip_ref = zipfile.ZipFile(file, 'r')\n",
        "      zip_ref.extractall()\n",
        "      zip_ref.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `'/content''\n",
            "/bin/bash: -c: line 0: `os.chdir('/content')'\n",
            ".config\n",
            "all-the-news.zip\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrIEmwrb6Jtt",
        "colab_type": "code",
        "outputId": "5a1b5eef-1e9b-4aa6-bdd2-da4063e9f667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T8K-Zn76nWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd sample_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fh91Piq6wY1",
        "colab_type": "code",
        "outputId": "b33bf461-c62d-4caa-b76c-4a6d86a01901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "!ls -a -l"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 903344\n",
            "drwxr-xr-x 1 root root      4096 Mar 25 05:17 .\n",
            "drwxr-xr-x 1 root root      4096 Mar 25 05:03 ..\n",
            "-rw-r--r-- 1 root root 255356300 Mar 25 05:17 all-the-news.zip\n",
            "-rw-r--r-- 1 root root 203539364 Mar 25 05:17 articles1.csv\n",
            "-rw-r--r-- 1 root root 225757056 Mar 25 05:17 articles2.csv\n",
            "-rw-r--r-- 1 root root 240344348 Mar 25 05:17 articles3.csv\n",
            "drwxr-xr-x 1 root root      4096 Mar 23 16:11 .config\n",
            "drwxr-xr-x 1 root root      4096 Mar 18 16:23 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLHbrFIq9mAB",
        "colab_type": "code",
        "outputId": "7104c255-e30f-4265-fa1a-88a7bb98cdf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "#for stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiCf6M3MAAB2",
        "colab_type": "code",
        "outputId": "5e053191-f77d-48a2-8b7d-2af13f33e0ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#lemmatizer code from https://simonhessner.de/lemmatize-whole-sentences-with-python-and-nltks-wordnetlemmatizer/\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "def nltk2wn_tag(nltk_tag):\n",
        "  if nltk_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif nltk_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif nltk_tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif nltk_tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:          \n",
        "    return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
        "  res_words = []\n",
        "  for word, tag in wn_tagged:\n",
        "    if tag is None:            \n",
        "      res_words.append(word)\n",
        "    else:\n",
        "      res_words.append(lemmatizer.lemmatize(word, tag))\n",
        "  return res_words\n",
        "\n",
        "\n",
        "import re\n",
        "# If you need to use the regex more than once it is suggested to compile it.\n",
        "notStartsWithAlphaNumber = re.compile(r\"[^\\w\\s]\")\n",
        "onlyDigits = re.compile(r\"\\d+\")\n",
        "\n",
        "def normalizeSentence(sentence):\n",
        "  #lowercase\n",
        "  #There's a risk of doing lower(). US and us are two different things\n",
        "  updatedSentence = sentence.lower()\n",
        "  #remove punctuations\n",
        "  updatedSentence = updatedSentence.translate(str.maketrans('','',string.punctuation))\n",
        "  updatedSentence = notStartsWithAlphaNumber.sub(\"\", updatedSentence)\n",
        "  updatedSentence = onlyDigits.sub(\"\", updatedSentence)  \n",
        "  return updatedSentence\n",
        "\n",
        "\n",
        "#not i am not removing stop words since i plan to use this for tf-idf calculation\n",
        "def normalizeAndLemmatizeSentence(sentence):\n",
        "  updatedSentence = normalizeSentence(sentence)\n",
        "  #lemmatization\n",
        "  return lemmatize_sentence(updatedSentence)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urre8EGZAtKC",
        "colab_type": "code",
        "outputId": "81cced97-07fd-47a4-ffdd-da887c804cde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "sentence_detection = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "#https://www.nltk.org/api/nltk.tokenize.html\n",
        "def extract_sentences(text):\n",
        "  return sentence_detection.tokenize(text)\n",
        "\n",
        "\n",
        "def extract_n_normalize_sentences(text):\n",
        "  allSentences = sentence_detection.tokenize(text)\n",
        "  updatedSentences = []\n",
        "  for sentence in allSentences:\n",
        "    #NOTE: You need an additional [] bracket before you add content to the list\n",
        "    #Reason = https://stackoverflow.com/questions/29947007/array-extendstring-adds-every-character-instead-of-just-the-string\n",
        "    updatedSentences.extend([normalizeSentence(sentence)])\n",
        "  return updatedSentences\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJZwBUl4geXW",
        "colab_type": "code",
        "outputId": "01b2e5a8-b7d2-41d1-feb5-4e38439ee70f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "normalizedSentences = []\n",
        "for file in os.listdir():\n",
        "    if(file.endswith('.csv')):\n",
        "      pdf = pd.read_csv(file)\n",
        "      print(f'Read file {file} into a panda dataframe')\n",
        "      \n",
        "      #this is a very slow method to iterate over the list and apply the normlize function.\n",
        "      #do somethign better: https://chrisalbon.com/python/basics/applying_functions_to_list_items/\n",
        "      #pdf['content'].apply(lambda content: normalizedSentences.extend(extract_n_normalize_sentences(content)))\n",
        "\n",
        "      pdf['content'].apply(lambda content: normalizedSentences.extend([normalizeSentence(s) for s in extract_sentences(content)]))\n",
        "      #for testing only, read only one file...\n",
        "      #break\n",
        "      #pdf.info()\n",
        "      #pdf.head(3)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read file articles1.csv into a panda dataframe\n",
            "Read file articles2.csv into a panda dataframe\n",
            "Read file articles3.csv into a panda dataframe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osldsRo8BN7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pdf = pd.read_csv('articles1.csv')\n",
        "# print(f'Read file articles1.csv into a panda dataframe')\n",
        "# pdf.info()\n",
        "# pdf.head(3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrU-rBxlBxAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pdf['content'][0]\n",
        "# normalizedSentences = []\n",
        "# pdf['content'].apply(lambda content: normalizedSentences.extend([content]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9129VwCNFglK",
        "colab_type": "code",
        "outputId": "579eefda-aa7e-4b3b-8c22-28b433d42ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "normalizedSentences[6749]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mr garfield said he read the script wept     mr doss had   rescued  men from a brutal battleground in japan     and that any hesitation he might have had vanished after he asked mr gibson about the controversy  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPLgcZbWJn4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('normalizedSentences.pickle', 'wb') as f:\n",
        "    pickle.dump(normalizedSentences, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gd52OpyJ4NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to make everything else run faster during testing\\development.\n",
        "normalizedSentences = normalizedSentences[:100000]\n",
        "with open('mini-normalizedSentences.pickle', 'wb') as f:\n",
        "    pickle.dump(normalizedSentences, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OZxK8uimN_u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2af9ed22-3d48-43d9-b632-4ae0c3b58ee7"
      },
      "source": [
        "!pwd\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JarSiodrk2uE",
        "colab_type": "code",
        "outputId": "7f7734c3-706e-417d-ab10-09be0e7616c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(normalizedSentences)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtKHf9X3ofQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}