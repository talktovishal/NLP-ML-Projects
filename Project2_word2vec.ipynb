{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2-word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN8fwvGGvkoKw64GaCRkhof",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talktovishal/NLP-ML-Projects/blob/master/Project2_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P3n1AKc08Dp",
        "colab_type": "code",
        "outputId": "55a7539b-aeab-4e43-9dac-423f344804e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install kaggle\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEQbNFfy3mqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /root/.kaggle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KitUzNZD4LcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo '{\"username\":\"thevishalchowdhary\",\"key\":\"0149faaacbbf52163a7ef279cd3264e3\"}' > /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdY9vZAs4eec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuBNNfql41F-",
        "colab_type": "code",
        "outputId": "4867fe4b-5c1d-461d-8e01-f1460ac70480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!kaggle datasets list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                                         title                                                size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  \n",
            "sudalairajkumar/novel-corona-virus-2019-dataset             Novel Corona Virus 2019 Dataset                     329KB  2020-03-05 06:14:42          19809  \n",
            "kimjihoo/coronavirusdataset                                 Coronavirus-Dataset                                  20KB  2020-03-05 01:57:50           4134  \n",
            "rupals/gpu-runtime                                          Segmentation GPU Kernel Performance Dataset           4MB  2020-03-01 10:04:27             56  \n",
            "anlthms/dfdc-video-faces                                    DFDC video face crops - parts 4-8                     2GB  2020-03-03 01:22:58             19  \n",
            "prakrutchauhan/indian-candidates-for-general-election-2019  Indian Candidates for General Election 2019         133KB  2020-03-03 07:01:53            198  \n",
            "brunotly/foreign-exchange-rates-per-dollar-20002019         Foreign Exchange Rates 2000-2019                      1MB  2020-03-03 17:43:07            273  \n",
            "shivamb/real-or-fake-fake-jobposting-prediction             [Real or Fake] Fake JobPosting Prediction            16MB  2020-02-29 08:23:34            321  \n",
            "tapakah68/yandextoloka-water-meters-dataset                 Water Meters Dataset                                982MB  2020-02-29 10:59:49             61  \n",
            "umangjpatel/pap-smear-datasets                              Pap Smear Datasets                                  448MB  2020-03-02 07:17:58             40  \n",
            "shank885/knife-dataset                                      Knife Dataset                                         1MB  2020-03-02 06:43:53             52  \n",
            "imdevskp/ebola-outbreak-20142016-complete-dataset           Ebola 2014-2016 Outbreak Complete Dataset           101KB  2020-02-26 14:36:31            353  \n",
            "imdevskp/sars-outbreak-2003-complete-dataset                SARS 2003 Outbreak Complete Dataset                  10KB  2020-02-26 10:25:22            277  \n",
            "gpiosenka/100-bird-species                                  110 Bird Species                                    757MB  2020-03-04 14:00:34            159  \n",
            "jessemostipak/hotel-booking-demand                          Hotel booking demand                                  1MB  2020-02-13 01:27:20           6385  \n",
            "tunguz/big-five-personality-test                            Big Five Personality Test                           159MB  2020-02-17 15:59:37           1846  \n",
            "arindam235/startup-investments-crunchbase                   StartUp Investments (Crunchbase)                      3MB  2020-02-17 21:54:42           1500  \n",
            "brendaso/2019-coronavirus-dataset-01212020-01262020         2019 Coronavirus dataset (January - February 2020)   53KB  2020-02-06 18:09:28           7068  \n",
            "jamzing/sars-coronavirus-accession                          SARS CORONAVIRUS ACCESSION                            2MB  2020-02-18 15:49:34           2030  \n",
            "timoboz/data-science-cheat-sheets                           Data Science Cheat Sheets                           596MB  2020-02-04 19:42:27           3129  \n",
            "brandenciranni/democratic-debate-transcripts-2020           Democratic Debate Transcripts 2020                  565KB  2020-02-27 00:07:40            476  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMvGYrT55pEz",
        "colab_type": "code",
        "outputId": "fcfcace7-8a34-4c10-fcd7-92d54bf06c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download snapcrack/all-the-news"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading all-the-news.zip to /content\n",
            " 98% 238M/244M [00:01<00:00, 131MB/s]\n",
            "100% 244M/244M [00:01<00:00, 137MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzKkqm9G5422",
        "colab_type": "code",
        "outputId": "94005d55-b1ff-41e6-e26c-6ad30a44b1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!os.chdir('/content')\n",
        "for file in os.listdir():\n",
        "    print(file)\n",
        "    if(file.endswith('.zip')):\n",
        "      zip_ref = zipfile.ZipFile(file, 'r')\n",
        "      zip_ref.extractall()\n",
        "      zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `'/content''\n",
            "/bin/bash: -c: line 0: `os.chdir('/content')'\n",
            ".config\n",
            "all-the-news.zip\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrIEmwrb6Jtt",
        "colab_type": "code",
        "outputId": "77c98d04-14e8-414d-8d11-ebb08f6e6f53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T8K-Zn76nWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cd sample_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fh91Piq6wY1",
        "colab_type": "code",
        "outputId": "f523e3b1-f384-4d8d-cd44-6ba6d6d3eed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!ls -a -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 903348\n",
            "drwxr-xr-x 1 root root      4096 Mar  5 07:07 .\n",
            "drwxr-xr-x 1 root root      4096 Mar  5 06:57 ..\n",
            "-rw-r--r-- 1 root root 255356300 Mar  5 07:07 all-the-news.zip\n",
            "-rw-r--r-- 1 root root 203539364 Mar  5 07:07 articles1.csv\n",
            "-rw-r--r-- 1 root root 225757056 Mar  5 07:07 articles2.csv\n",
            "-rw-r--r-- 1 root root 240344348 Mar  5 07:07 articles3.csv\n",
            "drwxr-xr-x 1 root root      4096 Mar  3 18:11 .config\n",
            "drwxr-xr-x 1 root root      4096 Mar  3 18:11 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLHbrFIq9mAB",
        "colab_type": "code",
        "outputId": "b664f816-035f-4bb4-d5de-7f6fabcba3dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "#for stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiCf6M3MAAB2",
        "colab_type": "code",
        "outputId": "4e122bc1-bf57-47ea-8e64-35dcb95c4c49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#lemmatizer code from https://simonhessner.de/lemmatize-whole-sentences-with-python-and-nltks-wordnetlemmatizer/\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "def nltk2wn_tag(nltk_tag):\n",
        "  if nltk_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif nltk_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif nltk_tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif nltk_tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:          \n",
        "    return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "  wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
        "  res_words = []\n",
        "  for word, tag in wn_tagged:\n",
        "    if tag is None:            \n",
        "      res_words.append(word)\n",
        "    else:\n",
        "      res_words.append(lemmatizer.lemmatize(word, tag))\n",
        "  return res_words\n",
        "\n",
        "def normalizeSentence(sentence):\n",
        "  #lowercase\n",
        "  updatedSentence = sentence.lower()\n",
        "  #remove punctuations\n",
        "  updatedSentence = updatedSentence.translate(str.maketrans('','',string.punctuation))\n",
        "  return updatedSentence\n",
        "\n",
        "\n",
        "#not i am not removing stop words since i plan to use this for tf-idf calculation\n",
        "def normalizeAndLemmatizeSentence(sentence):\n",
        "  #lowercase\n",
        "  updatedSentence = sentence.lower()\n",
        "  #remove punctuations\n",
        "  updatedSentence = updatedSentence.translate(str.maketrans('','',string.punctuation))\n",
        "  #lemmatization\n",
        "  return lemmatize_sentence(updatedSentence)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urre8EGZAtKC",
        "colab_type": "code",
        "outputId": "2b6ba39c-d4bd-41d0-ce34-dd7d6fb763f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "sentence_detection = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "#https://www.nltk.org/api/nltk.tokenize.html\n",
        "def extract_sentences(text):\n",
        "  return sentence_detection.tokenize(text)\n",
        "\n",
        "\n",
        "def extract_n_normalize_sentences(text):\n",
        "  allSentences = sentence_detection.tokenize(text)\n",
        "  updatedSentences = []\n",
        "  for sentence in allSentences:\n",
        "    #NOTE: You need an additional [] bracket before you add content to the list\n",
        "    #Reason = https://stackoverflow.com/questions/29947007/array-extendstring-adds-every-character-instead-of-just-the-string\n",
        "    updatedSentences.extend([normalizeSentence(sentence)])\n",
        "  return updatedSentences\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJZwBUl4geXW",
        "colab_type": "code",
        "outputId": "082cedb9-6cd5-474f-fe3e-a30db7420b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "normalizedSentences = []\n",
        "for file in os.listdir():\n",
        "    if(file.endswith('.csv')):\n",
        "      pdf = pd.read_csv(file)\n",
        "      print(f'Read file {file} into a panda dataframe')\n",
        "      \n",
        "      #this is a very slow method to iterate over the list and apply the normlize function.\n",
        "      #do somethign better: https://chrisalbon.com/python/basics/applying_functions_to_list_items/\n",
        "      #pdf['content'].apply(lambda content: normalizedSentences.extend(extract_n_normalize_sentences(content)))\n",
        "\n",
        "      pdf['content'].apply(lambda content: normalizedSentences.extend([normalizeSentence(s) for s in extract_sentences(content)]))\n",
        "      #pdf.info()\n",
        "      #pdf.head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read file articles2.csv into a panda dataframe\n",
            "Read file articles3.csv into a panda dataframe\n",
            "Read file articles1.csv into a panda dataframe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osldsRo8BN7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pdf = pd.read_csv('articles1.csv')\n",
        "# print(f'Read file articles1.csv into a panda dataframe')\n",
        "# pdf.info()\n",
        "# pdf.head(3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrU-rBxlBxAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pdf['content'][0]\n",
        "# normalizedSentences = []\n",
        "# pdf['content'].apply(lambda content: normalizedSentences.extend([content]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9129VwCNFglK",
        "colab_type": "code",
        "outputId": "5d6a1f4a-e50f-4992-cad7-b2a5db8d7163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "normalizedSentences[6749]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'will trump have the same patience as his generals'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7uuiOgsFkpb",
        "colab_type": "code",
        "outputId": "b5a99b00-62a4-4998-c28f-2a574aa20aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(normalizedSentences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5070143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nT3GJxJiuNB",
        "colab_type": "code",
        "outputId": "c216c81a-1587-4a9d-9596-23c4c2b928d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(pdf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OZxK8uimN_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to make everything else run faster during testing\\development.\n",
        "normalizedSentences = normalizedSentences[:50000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JarSiodrk2uE",
        "colab_type": "code",
        "outputId": "52b9bb24-2920-43a0-d0d6-71327b71fa03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(normalizedSentences)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1kx8E-KhKYQ",
        "colab_type": "code",
        "outputId": "344ec412-6865-4efb-8a9a-c1a0747f7e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%matplotlib inline\n",
        "#implement CBOW using pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#to have re-producable results\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb3aeedadf0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iobwIpUPhU_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PYArWx8iWLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "wordsInSentence = [word_tokenize(s) for s in normalizedSentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjvddUfkjrnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the vocab\n",
        "vocab = set()\n",
        "for words in wordsInSentence:\n",
        "  vocab.update(words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNlKwFRdnA9T",
        "colab_type": "code",
        "outputId": "76f3f213-1697-41ae-b83f-9b572834bace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36191"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl4YIEZXnHjp",
        "colab_type": "code",
        "outputId": "d0e5b80b-3d76-48c7-8f7d-f1d7b4d01bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import random\n",
        "for i, val in enumerate(random.sample(vocab, 10)):\n",
        "  print(val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chairman\n",
            "merger\n",
            "reserves\n",
            "moses\n",
            "thinly\n",
            "reprinted\n",
            "crystal\n",
            "outlook\n",
            "prominence\n",
            "minor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ55xi_LnVG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-70kE81nvu0",
        "colab_type": "code",
        "outputId": "1e115687-65b2-4074-8074-0a023a6ebe59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for raw_text in wordsInSentence:\n",
        "  for i in range(2, len(raw_text) - 2):\n",
        "      context = [\n",
        "                 word_to_ix[raw_text[i - 2]], \n",
        "                 word_to_ix[raw_text[i - 1]],\n",
        "                 word_to_ix[raw_text[i + 1]], \n",
        "                 word_to_ix[raw_text[i + 2]]\n",
        "                 ]\n",
        "      target = word_to_ix[raw_text[i]]\n",
        "      data.append((context, target))\n",
        "\n",
        "print(data[:5])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[([24417, 24305, 17752, 19501], 31045), ([24305, 31045, 19501, 14181], 17752), ([31045, 17752, 14181, 30909], 19501), ([17752, 19501, 30909, 1664], 14181), ([19501, 14181, 1664, 1538], 30909)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNevg8gkoTeD",
        "colab_type": "code",
        "outputId": "1cc46256-dd30-4ff8-b531-1e917e36ab5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000831"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v700obhQoaIw",
        "colab_type": "code",
        "outputId": "cd3a9e7c-4eb3-40a7-bf7b-6e504b0fa276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        pass\n",
        "\n",
        "# create your model and train.  here are some functions to help you make\n",
        "# the data ready for use by your module\n",
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix)  # example"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-8d2b75445397>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmake_context_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-8d2b75445397>\u001b[0m in \u001b[0;36mmake_context_vector\u001b[0;34m(context, word_to_ix)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_context_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-8d2b75445397>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_context_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 24417"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtKHf9X3ofQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}